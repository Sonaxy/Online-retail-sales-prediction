---
title:    "ISE 5103 Intelligent Data Analytics"
subtitle: "Homework 6 - Modeling Competition"
author:   "Daniel Carpenter, Sonaxy Mohanty, & Zachary Knepp"
date:     "October 2022"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    highlight: arrow
    latex_engine: xelatex
  # github_document:
  #   toc: yes
  #   toc_depth: 2
urlcolor: blue
cache: true
fig.width: 7
fig.height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r error=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# Packages --------

# Data Wrangling
library(tidyverse)
library(skimr)
library(lubridate) # dates

# Modeling
library(MASS)
library(caret) # Modeling variants like SVM
library(earth) # Modeling with Mars
library(pls)   # Modeling with PLS
library(glmnet) # Modeling with LASSO

# Aesthetics
library(knitr)
library(cowplot)  # multiple ggplots on one plot with plot_grid()
library(scales)
library(kableExtra)
library(ggplot2)
library(inspectdf)

#Hold-out Validation
library(caTools)

#Data Correlation
library(GGally)
library(regclass)

#RMSE Calculation
library(Metrics)

#p-value for OLS model
library(broom)

#ncvTest
library(car)
```

## General Data Prep
> For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

### Read Training Data
Clean data to ensure each read variable has the correct data type (factor, numeric, Date, etc.)
```{r, echo=FALSE}
# Convert all character data to factor
df.train.base <- read.csv('Train.csv', stringsAsFactors = TRUE)


# convert the ""'s to NA
df.train.base[df.train.base == ""] <- NA

# Clean data
df.train.base <- df.train.base %>% 
  
  # Ensure boolean variables are numeric
  mutate(adwordsClickInfo.isVideoAd = as.numeric(adwordsClickInfo.isVideoAd) ) %>%
  
  # Make sure dates are dates
  mutate(date = as.Date(date),
         visitStartTime = as_datetime(visitStartTime)
         ) %>%

  # Ensure factor are factors
  mutate(custId       = as.factor(custId),
         sessionId    = as.factor(sessionId),
         isTrueDirect = as.factor(isTrueDirect),
         newVisits    = as.factor(if_else(is.na(newVisits), 0, 1) ),
         bounces      = as.factor(if_else(is.na(bounces),   0, 1)   ),
         adwordsClickInfo.page      = as.factor(adwordsClickInfo.page),
         adwordsClickInfo.isVideoAd = as.factor(adwordsClickInfo.isVideoAd)
         ) %>%
  
  dplyr::select(-c(
    isMobile # This is contained in deviceCategory
    
  ))

#view(df.train.base)
```

### Create `numeric` and `factor` *base* `data frames`

Make data set of `numeric` variables called `df.train.base.numeric`
```{r, echo=FALSE}
df.train.base.numeric <- df.train.base %>%

  # selecting all the numeric data
  dplyr::select_if(is.numeric) %>%

  # converting the data frame to tibble
  as_tibble()
```

Make data set of `factor` variables called `df.train.base.factor`
```{r, echo=FALSE}
df.train.base.factor <- df.train.base %>%

  #selecting all the numeric data
  dplyr::select_if(is.factor) %>%

  #converting the data frame to tibble
  as_tibble()
```


## `(a, i)` - Data Understanding
> Create a data quality report of `numeric` and `factor` data  
> Created function called `dataQualityReport()` to create factor and numeric QA report

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Function for data report
dataQualityReport <- function(df) {
  
  # Function to remove any columns with NA
  removeColsWithNA <- function(df) {
    return( df[ , colSums(is.na(df)) == 0] )
  }
  
  # Create Comprehensive data report using skimr package
  # This is done a bit piece-wise because PDF latex does not like the skimr package
  # Very much. So Instead of printing `skim(df)`, I have to pull the contents manually
  # Unfortunately. This is not an issue with html typically.
  dataReport <- skim(df) %>%
    rename_all(~str_replace(.,"skim_","")) %>%
    arrange(type, desc(complete_rate) ) # sort data 
  
  # Filter to the class types
  dataReport.numeric <- dataReport %>% filter(type == 'numeric') # numeric data
  dataReport.factor  <- dataReport %>% filter(type == 'factor' ) # factor  data
  
  # Remove columns that do not apply to this type of data -----------------------
  
  ## numeric data
  dataReport.numeric <- removeColsWithNA(dataReport.numeric)  %>%
    
    # Clean column names by removing numeric prefix, 
    rename_all(~str_replace(.,"numeric.","")) 
    
  ## factor  data
  dataReport.factor  <- removeColsWithNA(dataReport.factor ) %>%
  
    # Clean column names by removing factor  prefix
    rename_all(~str_replace(.,"factor.",""))  
  
  
  # Set up options for Display the reports
  options(skimr_strip_metadata = FALSE)
  options(digits=2)
  options(scipen=99)
  
  # Numeric report <- Get summary of data frame --------------------------------
  
    # data frame stats
    dfStats.num <- data.frame(Num_Numeric_Variables = ncol(df %>% select_if(is.numeric)),
                              Total_Observations    = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.num <- dataReport.numeric %>% 
      dplyr::select(-type, -hist)
    
  
  # Factor report <- Get summary of data frame --------------------------------
  
    # Get summary of data frame
    dfStats.factor <- data.frame(Num_Factor_Variables = ncol(df %>% select_if(is.factor)),
                                 Total_Observations   = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.factor <- dataReport.factor  %>% 
      dplyr::select(-type, -ordered) 
    
    
  # Return the data frames
  return(list('dfStats.num'       = dfStats.num,    
              'dfColStats.num'    = dfColStats.num,
              'dfStats.factor'    = dfStats.factor, 
              'dfColStats.factor' = dfColStats.factor))
}
```

### Numeric Data Quality Report
* `pageviews` has some null values, but there are an insignificant amount, so we will just drop those rows.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Get the factor and numeric reports
initialReport <- dataQualityReport(df.train.base)

# Numeric data frame stats
initialReport$dfStats.num %>% kable()

# Numeric column stats
initialReport$dfColStats.num %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data
```

\newpage

### Factor Data Quality Report
* Location data unknown, so add an `Unknown` label for `null` values
* Appears that few people use website from the ads, which cause many null values. See more details below.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# factor data frame stats
initialReport$dfStats.factor %>% kable()

# factor column stats
initialReport$dfColStats.factor %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data
```

\newpage

### Exploratory Analysis
*    Need to predict a transformation of the `aggregate customer-level sales` value
based on the natural log  

```{r, echo=FALSE}
#aggregate revenue
CustRev <- stats::aggregate(df.train.base$revenue, 
                     by=list(df.train.base$custId),
                     FUN = sum,
                     na.rm = TRUE)

#renaming fields
names(CustRev) <- c('custId', 'totalRevenue')

#merging datasets
df.train.merge <- merge(df.train.base, CustRev, by='custId')

#applying transformation
df.train.merge$totalRevenue <- df.train.merge$totalRevenue + 1
df.train.merge$totalRevenue <- log(df.train.merge$totalRevenue)
```


#### Analysis 1: 
  
*   Checking the distribution of the transformation of the aggregrate customer-level sales value
based on the natural log:  

```{r, echo=FALSE}

hist(df.train.merge$totalRevenue,
     col = 'skyblue4',
     main = 'Distribution of Target Revenue for each customer',
     xlab = 'Target Revenue')
```
  
*   We can see that the transformed revenue doesn't look like a normal distribution with a spike at 0 revenue which means it can be an outlier.   
*   From the dataset, we can also see two sets of cutomers - one set who visited the site once and another who visited multiple times.  
*   The histogram here doesn't take into account the above fact and therefore the frequency of the target revenue is compromised.  
 

#### Analysis 2: 

```{r, echo=FALSE, fig.width=8}
df.train.merge %>%
  ggplot(aes(x = fct_reorder(channelGrouping, desc(totalRevenue) ),
             y = totalRevenue) ) +
  # Boxplots
  geom_boxplot(aes(color = channelGrouping), fill = 'lightsteelblue1', alpha = 0.7) +
  coord_flip() +
  
  # Theme, y scale format, and labels
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank()) +
  
  #scale_y_continuous(labels = comma) +
  labs(title = 'Distribution of Transformed Revenue by Different Online Store Channels',
       subtitle = 'Ordered Descending by Transformed Revenue Generated by Channels',
       x = 'Channels Used by Customers for Online Store',
       y = 'Transformed Revenue Generated')
```
  
*    The relevance behind this plot is to analyze whether the revenue generated is dependent on the channels via which the user came to the online store.  
*   Social media definitely plays an important role to attract customers to ttry online shopping.  

\newpage

## `(a, ii)` - Data Preparation
> For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

### Clean up Null Data

See that when `region` is `Osaka Prefecture` and `city` is `Osaka` some location details are `NULL` 

* Implication: the other fields can be manually set to correct values based on region and city criteria  
  
* So, set `location related` null fields to `know` description for the above `region` and `city` condition

```{r, echo=FALSE, results='hide'}

# df.train.base[!complete.cases(df.train.base$continent), ] %>%
#   distinct(continent, subContinent, country, region, metro, city)
# 
# 
# df.train.base %>%
#   filter(region == 'Osaka Prefecture') %>%
#   distinct(continent, subContinent, country, metro, city, region)

df.train <- df.train.merge


df.train$continent[is.na(df.train$continent) &
           df.train$region == 'Osaka Prefecture'] <- 'Asia'

# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(subContinent)

df.train$subContinent[is.na(df.train$subContinent) &
           df.train$region == 'Osaka Prefecture' &
             df.train$city == 'Osaka'] <- 'Eastern Asia'

# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(country)

df.train$country[is.na(df.train$country) &
           df.train$region == 'Osaka Prefecture' &
             df.train$city == 'Osaka'] <- 'Japan'
  
# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(metro)

# df.train %>%
#   filter(metro == 'JP_KINKI')

df.train$metro[is.na(df.train$metro) &
           df.train$region == 'Osaka Prefecture' &
             df.train$city == 'Osaka'] <- 'JP_KINKI'
```
  
See that when `continent` is `null`, then other `location` related fields are also null  

* Implication: these other fields depend on the `continent` variable  

* So, set `location related` null fields to `Unknow` description 

```{r, echo=FALSE, results='hide'}
# If null in location data, then 'Unknown' location
df.train <- df.train %>%
  mutate_at(
    # Only mutate these location variables
    vars(continent:city), 
    
    # Apply function rename null values to Unknown
    list(~ as.factor(ifelse(is.na(.), 'Unknown', .) ) ) 
  )
```

  
See that when `medium` is `null`, then other `ad`, `keyword` and `campaign` related fields are (mostly) null  

* Implication: these other fields depend on the `medium` variable

* So, set these null fields to `None` description, since a null value indicates
the user did not has `no traffic source`  

```{r, echo=FALSE, results='hide'}
# Now clean up the data in the main data frame `df.train`
# by setting null values to "No taffic source" if there is no medium
# Applies to "ad*", keyword, and campaign, referralPath, medium variables
df.train <- df.train %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the campaign variable
    vars(starts_with('ad'), keyword, campaign, referralPath, medium), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(medium), 'No traffic source ', .) ) ) 
  ) 
```

See that when `campaign` is `null`, then some `ad` related fields are (mostly) null  

* Implication: these other fields depend on the `campaign` variable

* So, set `adwordsClickInfo.page` null fields to `None` description, since a null value indicates
the user did not come using an advertisement  

```{r, echo=FALSE, results='hide'}
# Now clean up the data in the main data frame `df.train`
# by setting null values to "No Campaign" if there is no campaign.
# Applies to "ad*", keyword, and campaign variables
df.train <- df.train %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the campaign variable
    vars(adwordsClickInfo.page, adwordsClickInfo.slot, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, campaign), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(campaign), 'No Campaign', .) ) ) 
  ) 

```

Similar approach is done to impute the rest `NAs` in the categorical variables of the data set  
```{r, echo=FALSE, results='hide'}


# Now clean up the data in the main data frame `df.train`
# by setting null values to "No Keyword" if there is no keyword
# Applies to some "ad*", and keyword variables
df.train <- df.train %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the keyword variable
    vars(adContent, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, keyword), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(keyword), 'No Keyword', .) ) ) 
  ) 
```


```{r, echo=FALSE, results='hide'}
# If the `adContent` is null, label as `No Ad`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath, adContent), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adContent), 'No Ad', .) ) ) 
  )
```


```{r, echo=FALSE, results='hide'}
# If the `adwordsClickInfo.adNetworkType` is null, label as `No Ad Network`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(adwordsClickInfo.page, adwordsClickInfo.slot, adwordsClickInfo.gclId,
         adwordsClickInfo.isVideoAd, adwordsClickInfo.adNetworkType), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.adNetworkType), 'No Ad Network', .) ) ) 
  )
```

```{r, echo=FALSE, results='hide'}
# If the `adwordsClickInfo.page` is null, label as `No Ad Page`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath, adwordsClickInfo.slot, adwordsClickInfo.gclId, adwordsClickInfo.page), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.page), 'No Ad Page', .) ) ) 
  )
```


```{r, echo=FALSE, results='hide'}
# If the `network domain` is null, label as `No Domain`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(networkDomain:topLevelDomain), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(.), 'No Domain', .) ) ) 
  )
```


```{r, echo=FALSE, results='hide'}
# If the `network domain` is null, label as `No Referrer`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(referralPath), 'No Referrer', .) ) ) 
  )
```


```{r, echo=FALSE, results='hide'}
# If the `adwordsClickInfo.gclId` is null, label as `No Google Click ID`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(adwordsClickInfo.gclId), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.gclId), 'No Google Click ID', .) ) ) 
  )
```

Now we have very few null values rows. Let's simply remove them. See below for how many.
```{r, echo=FALSE}
# Number of rows with any nulls
numRowsWithNulls <- nrow(df.train[!complete.cases(df.train), ])

# Output text
paste('There are', numRowsWithNulls, 'rows with nulls')
paste0('That equates to ', round(numRowsWithNulls / nrow(df.train)* 100, 1), '% rows with nulls')

# Drop the rows
df.train <- df.train %>% drop_na()
paste('Total Rows Remaining:', nrow(df.train))
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report12 <- dataQualityReport(df.train)
# report12$dfColStats.factor %>% kable()
```

* We are going to factor collapse factor columns with more than 4 columns  
* So there will be 5 of the original, and 1 containing 'other'  

```{r, echo=FALSE}
# We are going to factor collapse factor columns with more than 4 columns
# So there will be 5 of the original, and 1 containing 'other'
# This is the threshold
FACTOR_THRESHOLD = 4

df.train.clean <- df.train

# Make data set of `factor` variables called `df.train.base.factor`
df.train.factor <- df.train %>%

  # selecting all the numeric data
  dplyr::select_if(is.factor) %>%

  # converting the data frame to tibble
  as_tibble()

# Get list of factors and the number of unique values
factorCols <-
  as.data.frame(t(df.train.factor %>% summarise_all(n_distinct))) #%>%
  # kable()

# Get a list of the factors we are going to collapse
colsWithManyFactors <- rownames(factorCols %>% filter(V1 > FACTOR_THRESHOLD))

# Show a summary of how many factors will be collapsed
numberOfColsWithManyFactors = length(colsWithManyFactors)
paste('Before cleaning, there are', numberOfColsWithManyFactors, 'factor columns with more than',
      FACTOR_THRESHOLD, 'unique values')

# Collapse the affected factors in the original data (the one that already has imputation)
## for each factor column that we are about to collapse
# The third column is omits the cutstomer ID and session ID
FIRST_NON_CUST_SESSION_IDX = 3
for (collapsedColNum in FIRST_NON_CUST_SESSION_IDX:numberOfColsWithManyFactors) {

  # The name of the column with null values
  nameOfThisColumn <- colsWithManyFactors[collapsedColNum]

  # Get the actual data of the column with nulls
  colWithManyFactors <- df.train[, nameOfThisColumn]

  # lumps all levels except for the n most frequent
  df.train.clean[, nameOfThisColumn] <- fct_lump_n(colWithManyFactors,
                                                   n=FACTOR_THRESHOLD)
}
# Check to see if the factor lumping worked
factorColsCleaned <-
  t(df.train.clean %>%
                       select_if(is.factor) %>%
                       summarise_all(n_distinct))
paste('After cleaning, there are', sum(factorColsCleaned > FACTOR_THRESHOLD + 1, na.rm = TRUE),
      "columns with more than", FACTOR_THRESHOLD + 1, "unique values (omitting NA's)")
```

\newpage

```{r, echo=FALSE}
#Cleaning up some  variables no longer needed
rm(CustRev,
   numRowsWithNulls,
   df.train.base,
   df.train.base.factor,
   df.train.base.numeric,
   df.train.merge,
   factorColsCleaned,
   FIRST_NON_CUST_SESSION_IDX,
   nameOfThisColumn,
   numberOfColsWithManyFactors,
   collapsedColNum,
   colsWithManyFactors
)
```


### Group by Customer

Get list of customers who visited once and twice
```{r, echo=FALSE, results='hide'}
# Get list of customers and visit count
df.train.clean.uniqueCust <- df.train.clean %>%
  group_by(custId) %>%
  summarise(totalVisits = n() )


# Customer visits more than 1
df.train.clean.uniqueCustMulti <- df.train.clean.uniqueCust %>%
  filter(totalVisits > 1)

# Customer data who only visited once
df.train.clean.custSingle <- df.train.clean %>%
  filter( !(custId %in% df.train.clean.uniqueCustMulti$custId) )

# Number of customers visiting more than once
nrow(df.train.clean.uniqueCustMulti)
```

Group by customer & Sum up all numeric data

* Filter to only the customers who visited twice  

* Get the unique visits and choose the first visit  

* THis is just an assumption! Not the best, but we have to make a choice.  

* Append unique customers to non-unique customers (that are now unique)  

* Note not using all columns, only columns NOT specific to the model

```{r, echo=FALSE}
df.train.clean.custMultiToSingle <- df.train.clean %>%
  
  # Filter to only the customers who visited twice
  filter(custId %in% df.train.clean.uniqueCustMulti$custId) %>%
    
  # Note Remove session related variables and regroup
  group_by(custId) %>%
  
  # Get the unique visits and choose the first visit
  # THis is just an assumption! Not the best, but we have to make a choice.
  summarise(
    browser                        = unique(browser)[1], 
    operatingSystem                = unique(operatingSystem)[1], 
    deviceCategory                 = unique(deviceCategory)[1], 
    continent                      = unique(continent)[1], 
    subContinent                   = unique(subContinent)[1], 
    country                        = unique(country)[1], 
    region                         = unique(region)[1], 
    metro                          = unique(metro)[1], 
    city                           = unique(city)[1], 
    networkDomain                  = unique(networkDomain)[1], 
    topLevelDomain                 = unique(topLevelDomain)[1], 
    campaign                       = unique(campaign)[1], 
    source                         = unique(source)[1],
    medium                         = unique(medium)[1],
    keyword                        = unique(keyword)[1], 
    isTrueDirect                   = unique(isTrueDirect)[1], 
    referralPath                   = unique(referralPath)[1], 
    adContent                      = unique(adContent)[1], 
    adwordsClickInfo.page          = unique(adwordsClickInfo.page)[1], 
    adwordsClickInfo.slot          = unique(adwordsClickInfo.slot)[1], 
    adwordsClickInfo.gclId         = unique(adwordsClickInfo.gclId)[1], 
    adwordsClickInfo.adNetworkType = unique(adwordsClickInfo.adNetworkType)[1], 
    adwordsClickInfo.isVideoAd     = unique(adwordsClickInfo.isVideoAd)[1],
    bounces                        = unique(bounces)[1],
    newVisits                      = unique(newVisits)[1],
    pageviews                      = sum(pageviews),
    revenue                        = sum(revenue)
  )


df.train.clean.custSingle <- df.train.clean.custSingle %>% 
  dplyr::select(
          custId, browser, operatingSystem, deviceCategory, continent, subContinent, country, 
          region, metro, city, networkDomain, topLevelDomain, campaign, source, medium, 
          keyword, isTrueDirect, referralPath, adContent, adwordsClickInfo.page, 
          adwordsClickInfo.slot, adwordsClickInfo.gclId, adwordsClickInfo.adNetworkType, 
          adwordsClickInfo.isVideoAd, bounces, newVisits, pageviews, revenue
          )

# Make sure we captured all customers
nrow(df.train.clean.custSingle) + nrow(df.train.clean.custMultiToSingle)
nrow(df.train.clean.uniqueCust)

ncol(df.train.clean.custSingle)
ncol(df.train.clean.custMultiToSingle)

# Append unique customers to non-unique customers (that are now unique)
df.train.clean.cust <- rbind(df.train.clean.custMultiToSingle, 
                             df.train.clean.custSingle)  
```

### Create `targetRevenue` Variable
```{r}
df.train.clean.cust <- df.train.clean.cust %>%
  mutate(targetVariable = log(revenue + 1)) %>%
  dplyr::select(-revenue)
```

### Then create dataset without the `custID` field called `df.train.clean.noCust`
```{r, echo=FALSE}
df.train.clean.noCust <- df.train.clean.cust[, 2:ncol(df.train.clean.cust)] 
```

\newpage

## `(a, iii)` - Modeling

### OLS Model

#### Fit the Model  

* Initially created a model with all variables, then used `stepAIC()` to identify important variables  
* Implemented in the OLS model to realize a better fit model.


```{r, cache=TRUE}
# The OLS model
# See RMD for stepAIC function that generated these relevant variables for the model
ols <- lm(targetVariable ~ operatingSystem + country + metro + city + networkDomain + 
            source + keyword + isTrueDirect + referralPath + bounces + 
            newVisits + pageviews,
          data = df.train.clean.noCust)
```  
  
```{r, echo=FALSE, results='hide'}
# Key diagnostics for OLS: lm final summary table
summary(ols)
#plot(ols)
```  
  
```{r, echo=FALSE, results='hide'}
# Note that used Step AIC to build out the abovve model.
# Commented out to minimize computation exertion in file

# ols.stepAIC <- stepAIC(ols, direction = "both")
# summary(ols.stepAIC)

# Best Model Output:
# targtargetVariable ~ operatingSystem + country + metro + city + networkDomain + 
#     source + keyword + isTrueDirect + referralPath + bounces + 
#     newVisits + pageviewsnewVisits + pageviews
```  

#### View and Interpret Results  
  

```{r, echo=FALSE}
# Get the RMSE and R Squared of the model
ols.rmse    <- rmse(actual=df.train.clean.noCust$targetVariable, predicted=ols$fitted.values)
ols.summary <- summary(ols)

# Key diagnostics
keyDiagnostics.ols <- data.frame(Model    = 'OLS',
                                 Notes    = 'lm',
                                 Hyperparameters = 'N/A',
                                 RMSE     = ols.rmse,
                                 Rsquared = ols.summary$adj.r.squared)

# Show output
keyDiagnostics.ols %>% 
  knitr::kable()
``` 
  
* Comparing the OLS model with various other robust models to see how better these robust models perform as compared to the OLS model based on the RMSE and $R^2$.


### Model 2: PCR Model

#### Fit the Model  

* Based on model testing, highest $R^2$ is around 68 number of components.  
* Fits data much better than the former model.  

```{r, echo=FALSE, results='hide', cache=TRUE}
# Fit the model
pcr <- mvr(targetVariable ~ operatingSystem + country + metro + city + networkDomain + 
            source + keyword + isTrueDirect + referralPath + bounces + 
            newVisits + pageviews,
          data = df.train.clean.noCust, 
          
          # Modeling params
          center = TRUE,
          scale  = TRUE, 
          validation = "CV")
```  
  

```{r, echo=FALSE, results='hide'}
# See the summary output
summary(pcr)

# validationplot(pcr)
# validationplot(pcr, val.type = 'R2')
```

#### View and Interpret Results  
  

```{r, echo=FALSE}
# Key diagnostics for PCR final summary table
RMSE.pcr <- RMSEP(pcr, ncomp=15)
R2.pcr <- R2(pcr, ncomp = 1:15)

# Get the RMSE and R Squared of the model
keyDiagnostics.pcr <- data.frame(Model    = 'PCR',
                                 Notes    = 'pcr',
                                 Hyperparameters = paste('ncomp = ', pcr$ncomp),
                                 RMSE     = min(RMSE.pcr$val),
                                 Rsquared = max(R2.pcr$val) )

# Show output
keyDiagnostics.pcr %>% 
  knitr::kable()
```  
  
* 28 components explain 100% variance in the data set, but 15 components are enough to justify more than 75% of data variance.  
*   We will see if MARS and ELasticNet models outperform the PCR model.  

### Model 3: MARS

#### Fit the Model  

* Use MARS model from earth package.  
* Fits data similarly to the former models.  

```{r, echo=FALSE, results='hide', cache=TRUE}

# Model tuning controls
ctrl <- trainControl(method  = "repeatedcv", 
                     number  = 5, # 5 fold cross validation
                     repeats = 1  # 1 repeats
                     )

# Fit the model
marsFit <- train(data = df.train.clean.noCust, 
                 targetVariable ~ operatingSystem + country + metro + city + networkDomain + 
                   source + keyword + isTrueDirect + referralPath + bounces + 
                   newVisits + pageviews,
                  method     = "earth",             # Earth is for MARS models
                  tuneLength = 9,                   # 9 values of the cost function
                  preProc    = c("center","scale"), # Center and scale data
                  trControl  = ctrl 
                 )
summary(marsFit)
#plot(marsFit)
```

#### View and Interpret Results  
  
```{r, echo=FALSE}
# Key diagnostics for final model

# Get the RMSE and R Squared of the model
hyperparameters.mars = list('degree' = marsFit[["bestTune"]][["degree"]],
                            'nprune' = marsFit[["bestTune"]][["nprune"]])

keyDiagnostics.mars <- data.frame(Model   = 'MARS',
                                  Notes    = 'caret and earth',
                                  Hyperparameters = paste('Degree =', hyperparameters.mars$degree, ',',
                                                          'nprune =', hyperparameters.mars$nprune)
                                  )

keyDiagnostics.mars <- cbind(keyDiagnostics.mars,
                            marsFit$results %>% 
                              filter(degree == hyperparameters.mars$degree,
                                     nprune == hyperparameters.mars$nprune) %>%
                              dplyr::select(RMSE, Rsquared)
                      )

# Show output
keyDiagnostics.mars %>% kable()
```
  
*    See that the model overall performs well, and in fact performs better as compared to the PCR model (in terms of RMSE and $R^2$).

### Model 4: Elastic Net Model

#### Fit the Model  

```{r, warning=FALSE, message=FALSE, echo=FALSE}
rm(df.train.clean.factor)
rm(df.train.clean.numeric)

# Train and tune the Elastic net
# Fit the model
fit.elasticnet <- train(data = df.train.clean.noCust, 
                        targetVariable ~ operatingSystem + country + metro + city + networkDomain + 
                          source + keyword + isTrueDirect + referralPath + bounces + 
                          newVisits + pageviews,
                        method     = "glmnet",            # Elastic net
                        preProc    = c("center","scale"), # Center and scale data
                        tuneLength = 10,                  # 10 values of alpha and lambdas
                        trControl  = ctrl)
```  
  
```{r, echo=FALSE, results='hide'}
# Function to get the best hypertuned parameters
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
result.elasticnet <- get_best_result(fit.elasticnet)
``` 
  
#### View and Interpret Results  
  

```{r, echo=FALSE}
# Gather key diagnostics for summary table
# Get the RMSE and R Squared of the model
hyperparameters.elasticnet = list('Alpha'  = result.elasticnet$alpha,
                                  'Lambda' = result.elasticnet$lambda)


keyDiagnostics.elasticnet <- data.frame(Model    = 'Elastic Net',
                                        Notes    = 'caret and elasticnet',
                                        Hyperparameters = paste('Alpha =',
                                                                hyperparameters.elasticnet$Alpha, ',',
                                                                'Lambda =',
                                                                hyperparameters.elasticnet$Lambda),
                                        RMSE     = result.elasticnet$RMSE,
                                        Rsquared = result.elasticnet$Rsquared
                                        )

# Show output
keyDiagnostics.elasticnet %>% knitr::kable()
```
  
* The Elastic Net Model performs similar to PCR model.  

* Thus, MARS model outperformed all the rest models based on the `RMSE` and $R^2$ measures.  
  
*   We will now predict the test set with the MARS model that we created.  


\newpage

## `(a, iv)` - Debrief

### Summary Table  

```{r, echo=FALSE}
# Add the key diagnostics here
rbind(
  keyDiagnostics.ols,
  keyDiagnostics.pcr,
  keyDiagnostics.mars,
  keyDiagnostics.elasticnet
) %>%
  
  # Round to 4 digits across numeric data
  mutate_if(is.numeric, round, digits = 4) %>%
  
  # Spit out kable table
  kable()
```

### Interpretations of Debrief  

*   For MARS model, we used `caret` and `earth` method  
*   The hyperparameters that worked best for the model is` Degree = 1` , `nprune = 8`  
*   We first imputed all the `NAs` we had in the data set and got the final dataset of `69753` rows, i.e., we only dropped about 0.5% rows with nulls in order to capture the sanctity of the dataset and performed transformation of revenue on this dataset  
*   Since after imputation, there were a large number of dummy categorical values, so initially when the model were tested it took a lot of time and for some cases the model didn't even fit  
*   So, factor collapsing was performed on these factor columns which gave us 5 of the `original` values and 1 `other`  
*   Same steps were performed on the test data set  
*   After that even, the `RMSE` and $R^2$ values were not upto the mark for the models 
*   So, came up with the idea of grouping of customers, i.e., grouping based on which customers visited once or twice  
*   Then the transformation of revenue, `targetRevenue` was done on this grouped data set, and then  `custId` column was removed from the dataset to avoid overfitting of the model  
*   Then the performance of the models boosted up 

\newpage

# Apply to Test Data  

* Need to clean test data like we did in the train  

* Note all comments for the main model apply here  

* Then apply the models to this dataset

* Outputs a CSV with predicted customer log revenue

* For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# ### Read Testing Data
# Clean data to ensure each read variable has the correct data type (factor, numeric, Date, etc.)
# Convert all character data to factor
df.test.base <- read.csv('Test.csv', stringsAsFactors = TRUE)


# convert the ""'s to NA
df.test.base[df.test.base == ""] <- NA

# Clean data
df.test.base <- df.test.base %>% 
  
  # Ensure boolean variables are numeric
  mutate(adwordsClickInfo.isVideoAd = as.numeric(adwordsClickInfo.isVideoAd) ) %>%
  
  # Make sure dates are dates
  mutate(date = as.Date(date),
         visitStartTime = as_datetime(visitStartTime)
         ) %>%

  # Ensure factor are factors
  mutate(custId       = as.factor(custId),
         sessionId    = as.factor(sessionId),
         isTrueDirect = as.factor(isTrueDirect),
         newVisits    = as.factor(if_else(is.na(newVisits), 0, 1) ),
         bounces      = as.factor(if_else(is.na(bounces),   0, 1)   ),
         adwordsClickInfo.page      = as.factor(adwordsClickInfo.page),
         adwordsClickInfo.isVideoAd = as.factor(adwordsClickInfo.isVideoAd)
         ) %>%
  
  dplyr::select(-c(
    isMobile # This is contained in deviceCategory
    
  ))

#view(df.test.base)

### Create `numeric` and `factor` *base* `data frames`
# Make data set of `numeric` variables called `df.test.base.numeric`
df.test.base.numeric <- df.test.base %>%

  # selecting all the numeric data
  dplyr::select_if(is.numeric) %>%

  # converting the data frame to tibble
  as_tibble()

# Make data set of `factor` variables called `df.test.base.factor`
df.test.base.factor <- df.test.base %>%

  #selecting all the numeric data
  dplyr::select_if(is.factor) %>%

  #converting the data frame to tibble
  as_tibble()

# ### Numeric Data Quality Report
# * `pageviews` has some null values, but there are an insignificant amount, so we will just drop those rows.

# Get the factor and numeric reports
initialReport <- dataQualityReport(df.test.base)

# Numeric data frame stats
initialReport$dfStats.num %>% kable()

# Numeric column stats
initialReport$dfColStats.num %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data

# ### Factor Data Quality Report
# * Location data unknown, so add an `Unknown` label for `null` values
# * Appears that few people use website from the ads, which cause many null values. See more details below.

# factor data frame stats
initialReport$dfStats.factor %>% kable()

# factor column stats
initialReport$dfColStats.factor %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data

# `(a, ii)` - Data Preparation
# > For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.
# 
# ### Clean up Null Data
# See that when `region` is `Osaka Prefecture` and `city` is `Osaka` some location details are `NULL` 
# * Implication: the other fields can be manually set to correct values based on region and city criteria  
# * So, set `location related` null fields to `know` description for the above `region` and `city` condition

df.test <- df.test.base


df.test$continent[is.na(df.test$continent) &
           df.test$region == 'Osaka Prefecture'] <- 'Asia'

# df.test %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(subContinent)

df.test$subContinent[is.na(df.test$subContinent) &
           df.test$region == 'Osaka Prefecture' &
             df.test$city == 'Osaka'] <- 'Eastern Asia'

# df.test %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(country)

df.test$country[is.na(df.test$country) &
           df.test$region == 'Osaka Prefecture' &
             df.test$city == 'Osaka'] <- 'Japan'
  
# df.test %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(metro)

# df.test %>%
#   filter(metro == 'JP_KINKI')

df.test$metro[is.na(df.test$metro) &
           df.test$region == 'Osaka Prefecture' &
             df.test$city == 'Osaka'] <- 'JP_KINKI'

# <!-- See that when `continent` is `null`, then other `location` related fields are also null   -->
# <!-- * Implication: these other fields depend on the `continent` variable   -->
# <!-- * So, set `location related` null fields to `Unknow` description  -->

# If null in location data, then 'Unknown' location
df.test <- df.test %>%
  mutate_at(
    # Only mutate these location variables
    vars(continent:city), 
    
    # Apply function rename null values to Unknown
    list(~ as.factor(ifelse(is.na(.), 'Unknown', .) ) ) 
  )
  
# See that when `medium` is `null`, then other `ad`, `keyword` and `campaign` related fields are (mostly) null  
# * Implication: these other fields depend on the `medium` variable
# * So, set these null fields to `None` description, since a null value indicates
# the user did not has `no traffic source`  

# Now clean up the data in the main data frame `df.test`
# by setting null values to "No taffic source" if there is no medium
# Applies to "ad*", keyword, and campaign, referralPath, medium variables
df.test <- df.test %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the campaign variable
    vars(starts_with('ad'), keyword, campaign, referralPath, medium), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(medium), 'No traffic source ', .) ) ) 
  ) 

# See that when `campaign` is `null`, then some `ad` related fields are (mostly) null  
# * Implication: these other fields depend on the `campaign` variable
# * So, set `adwordsClickInfo.page` null fields to `None` description, since a null value indicates
# the user did not come using an advertisement  

# Now clean up the data in the main data frame `df.test`
# by setting null values to "None" if there is no campaign.
# Applies to "ad*", keyword, and campaign variables
df.test <- df.test %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the campaign variable
    vars(adwordsClickInfo.page, adwordsClickInfo.slot, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, campaign), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(campaign), 'No Campaign', .) ) ) 
  ) 


# Similar to campaign, whenever `keyword` is NA, some `ads` is null  
#NO_KEYWORD_TEXT = 'No Keyword'

# Now clean up the data in the main data frame `df.test`
# by setting null values to "No Keyword" if there is no keyword
# Applies to some "ad*", and keyword variables
df.test <- df.test %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the keyword variable
    vars(adContent, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, keyword), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(keyword), 'No Keyword', .) ) ) 
  ) 

# Similar to the campaign data, if the `adContent` is null, label as `No Ad`. 
# *   Implications: If there is no ad Content of the traffic source then there is no no referral path  

# If the `adContent` is null, label as `None`
df.test <- df.test %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath, adContent), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adContent), 'No Ad', .) ) ) 
  )

# Similar to the campaign data, if the `adwordsClickInfo.adNetworkType` is null, then all `ad` related variables are also `NULL`. 
# *   Implications: If there is no ad search then customer didn't see any ad.  

# If the `adwordsClickInfo.adNetworkType` is null, label as `No Ad Network`
df.test <- df.test %>%
  mutate_at(
    # Only mutate the referral path
    vars(adwordsClickInfo.page, adwordsClickInfo.slot, adwordsClickInfo.gclId,
         adwordsClickInfo.isVideoAd, adwordsClickInfo.adNetworkType), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.adNetworkType), 'No Ad Network', .) ) ) 
  )

# Similar to the adwordsClickInfo.adNetworkType data, if the `adwordsClickInfo.page` is null, then some `ad` related variables are also `NULL` and there is no referral source. 
# *   Implications: If there is no ad published on a page then customer didn't see any ad.  

# If the `adwordsClickInfo.page` is null, label as `No Ad Page`
df.test <- df.test %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath, adwordsClickInfo.slot, adwordsClickInfo.gclId, adwordsClickInfo.page), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.page), 'No Ad Page', .) ) ) 
  )

# If `network domain` is `NULL` then all the related domains are also NULL. 
df.test <- df.test %>%
  mutate_at(
    # Only mutate the referral path
    vars(networkDomain:topLevelDomain), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(.), 'No Domain', .) ) ) 
  )

# Setting `referralPath` for NAs. 
# If the `network domain` is null, label as `No Domain`
df.test <- df.test %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(referralPath), 'No Referrer', .) ) ) 
  )

# Setting `adwordsClickInfo.gclId` for NAs. 
# If the `network domain` is null, label as `No Domain`
df.test <- df.test %>%
  mutate_at(
    # Only mutate the referral path
    vars(adwordsClickInfo.gclId), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.gclId), 'No Google Click ID', .) ) ) 
  )

# Now we have very few null values rows. Let's simply remove them. See below for how many.
# Number of rows with any nulls
numRowsWithNulls <- nrow(df.test[!complete.cases(df.test), ])

# Output text
paste('There are', numRowsWithNulls, 'rows with nulls')
paste0('That equates to ', round(numRowsWithNulls / nrow(df.test)* 100, 1), '% rows with nulls')

# Drop the rows
# df.test <- df.test %>% drop_na()
paste('Total Rows Remaining:', nrow(df.test))

# Get list of unique factors in the training sample
# * Goal: reduce factors in test data to the training sample  
# * Models cannot predict factors that don't exist in the model  
df.test.clean <- df.test # copy data


# Get list of unique factors in the training sample
uniqueTrainFactors <- lapply(df.train.clean.noCust %>% select_if(is.factor), unique)

# For each factor column in the train data
for (factorColName in names(uniqueTrainFactors) ) {
  
  # Get unique levels of the factor column
  trainUniqueFactors <- unique(df.train.clean.noCust[, factorColName])
  trainUniqueFactors <- levels(trainUniqueFactors[[factorColName]])

  # If the factor data is in training data, then use it, else put 'Other'
  df.test.clean[[factorColName]] = as.factor(ifelse(df.test.clean[[factorColName]] %in% trainUniqueFactors,
                                                    paste0(df.test.clean[[factorColName]]),
                                                    'Other') 
  )
}

# If any remaining nulls then bin as other or  0 
# checkNAs <- function(x) { sum(is.na(x))}
# lapply(df.test.clean, checkNAs)
# lapply(df.test.clean, class)
# See that pageviews, operatingSystem, and source are NA

df.test.clean$pageviews[is.na(df.test.clean$pageviews)] <- 0
df.test.clean$operatingSystem[is.na(df.test.clean$operatingSystem)] <- "Other"
df.test.clean$source[is.na(df.test.clean$source)] <- "Other"

# Check to see if it worked
# check <- df.test.clean[!complete.cases(df.test.clean), ]
# View(check)

# Validation
uniqueTestFactors <- lapply(df.test.clean %>% select_if(is.factor), unique)
sum(!is.element(uniqueTrainFactors$operatingSystem, 
                 uniqueTestFactors$operatingSystem)
)

# uniqueTrainFactors$operatingSystem
# uniqueTestFactors$operatingSystem
# uniqueTestFactors

### Group by Customer
## Get list of customers who visited once and twice
# Get list of customers and visit count
df.test.clean.uniqueCust <- df.test.clean %>%
  group_by(custId) %>%
  summarise(totalVisits = n() )


# Customer visits more than 1
df.test.clean.uniqueCustMulti <- df.test.clean.uniqueCust %>%
  filter(totalVisits > 1)

# Customer data who only visited once
df.test.clean.custSingle <- df.test.clean %>%
  filter( !(custId %in% df.test.clean.uniqueCustMulti$custId) )

# Number of customers visiting more than once
nrow(df.test.clean.uniqueCustMulti)

# Group by customer & Sum up all numeric data
# * Filter to only the customers who visited twice  
# * Get the unique visits and choose the first visit  
# * THis is just an assumption! Not the best, but we have to make a choice.  
# * Append unique customers to non-unique customers (that are now unique)  
# * Note not using all columns, only columns NOT specific to the model

df.test.clean.custMultiToSingle <- df.test.clean %>%
  
  # Filter to only the customers who visited twice
  filter(custId %in% df.test.clean.uniqueCustMulti$custId) %>%
    
  # Note Remove session related variables and regroup
  group_by(custId) %>%
  
  # Get the unique visits and choose the first visit
  # This is just an assumption! Not the best, but we have to make a choice.
  summarise(
    browser                        = unique(browser)[1], 
    operatingSystem                = unique(operatingSystem)[1], 
    deviceCategory                 = unique(deviceCategory)[1], 
    continent                      = unique(continent)[1], 
    subContinent                   = unique(subContinent)[1], 
    country                        = unique(country)[1], 
    region                         = unique(region)[1], 
    metro                          = unique(metro)[1], 
    city                           = unique(city)[1], 
    networkDomain                  = unique(networkDomain)[1], 
    topLevelDomain                 = unique(topLevelDomain)[1], 
    campaign                       = unique(campaign)[1], 
    source                         = unique(source)[1],
    medium                         = unique(medium)[1],
    keyword                        = unique(keyword)[1], 
    isTrueDirect                   = unique(isTrueDirect)[1], 
    referralPath                   = unique(referralPath)[1], 
    adContent                      = unique(adContent)[1], 
    adwordsClickInfo.page          = unique(adwordsClickInfo.page)[1], 
    adwordsClickInfo.slot          = unique(adwordsClickInfo.slot)[1], 
    adwordsClickInfo.gclId         = unique(adwordsClickInfo.gclId)[1], 
    adwordsClickInfo.adNetworkType = unique(adwordsClickInfo.adNetworkType)[1], 
    adwordsClickInfo.isVideoAd     = unique(adwordsClickInfo.isVideoAd)[1],
    bounces                        = unique(bounces)[1],
    newVisits                      = unique(newVisits)[1],
    pageviews                      = sum(pageviews)
  )


df.test.clean.custSingle <- df.test.clean.custSingle %>% 
  dplyr::select(
          custId, browser, operatingSystem, deviceCategory, continent, subContinent, country, 
          region, metro, city, networkDomain, topLevelDomain, campaign, source, medium, 
          keyword, isTrueDirect, referralPath, adContent, adwordsClickInfo.page, 
          adwordsClickInfo.slot, adwordsClickInfo.gclId, adwordsClickInfo.adNetworkType, 
          adwordsClickInfo.isVideoAd, bounces, newVisits, pageviews
          )

# Make sure we captured all customers
nrow(df.test.clean.custSingle) + nrow(df.test.clean.custMultiToSingle)
nrow(df.test.clean.uniqueCust)

ncol(df.test.clean.custSingle)
ncol(df.test.clean.custMultiToSingle)

# Append unique customers to non-unique customers (that are now unique)
df.test.clean.cust <- rbind(df.test.clean.custMultiToSingle, 
                             df.test.clean.custSingle)  

### Create dataset without the `custID` field called `df.test.clean.noCust`
df.test.clean.noCust <- df.test.clean.cust[, 2:ncol(df.test.clean.cust)] 
```

```{r, echo=FALSE}
## Predict the customer data using the MARS model
predictions <- as.vector(
                    predict(marsFit,                # MARS Model from training data
                    newdata = df.test.clean.noCust) # Test data 
                    )


outputData <- data.frame(custId      = df.test.clean.cust$custId,
                         predRevenue = predictions)

# write the file
write.csv(file      = 'Kaggle Submission Data (Test).csv',
          x         = outputData,
          row.names = FALSE)
```
